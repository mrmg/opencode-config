{
  // ============================================================================
  // AGENT TIER STRATEGY
  // ============================================================================
  // 1. Kato/Work: Anthropic only (Claude Opus/Sonnet via GitHub Copilot or direct)
  // 2. Personal: Mix of GitHub, Gemini, OpenAI, Cursor (daily driver)
  // 3. Ollama: Private local models (100% private, limited 16GB context)
  // 4. Free: OpenCode free tier (fallback when budget conscious)
  // ============================================================================
  
  "agents": {
    
    // ========================================================================
    // KATO / WORK AGENTS (Anthropic Only)
    // ========================================================================
    "__comment_kato": "Work projects - Anthropic Claude models only (OpenCode Black)",
    
    "Kato-Architect": {
      "model": "opencode/claude-opus-4-5",
      "temperature": 0.2,
      "mode": "primary",
      "description": "Kato: ARCHITECT - Claude Opus 4.5 (80.9% SWE-bench)"
    },
    
    "Kato-Coder": {
      "model": "opencode/claude-sonnet-4-5",
      "temperature": 0.2,
      "mode": "primary",
      "description": "Kato: BALANCED CODER - Claude Sonnet 4.5 (77.2% SWE-bench)"
    },
    
    "Kato-Strategist": {
      "model": "opencode/claude-opus-4-5",
      "temperature": 0.2,
      "mode": "primary",
      "description": "Kato: STRATEGIC PLANNING - Claude Opus 4.5"
    },
    
    "kato-reviewer": {
      "model": "opencode/claude-sonnet-4-5",
      "temperature": 0.2,
      "mode": "subagent",
      "description": "Kato: Code review and quality assurance"
    },
    
    "kato-documenter": {
      "model": "opencode/claude-sonnet-4-5",
      "temperature": 0.3,
      "mode": "subagent",
      "description": "Kato: Technical documentation for work projects"
    },
    
    // ========================================================================
    // COPILOT AGENTS (GitHub Copilot Ecosystem)
    // ========================================================================
    "__comment_copilot": "GitHub Copilot ecosystem agents",
    
    "CoPilot-Architect": {
      "model": "github-copilot/gpt-5.2",
      "temperature": 0.2,
      "mode": "primary",
      "description": "CoPilot: ARCHITECT - GPT-5.2 (80% SWE-bench)"
    },
    
    "CoPilot-Coder": {
      "model": "github-copilot/claude-sonnet-4.5",
      "temperature": 0.2,
      "mode": "primary",
      "description": "CoPilot: CODER - Claude Sonnet 4.5 (77.2% SWE-bench)"
    },
    
    "Personal-Coder-Flash": {
      "model": "github-copilot/gpt-5-mini",
      "temperature": 0.2,
      "mode": "primary",
      "description": "Personal: Fast coder - GPT-5 mini"
    },
    
    "copilot-reasoner": {
      "model": "github-copilot/gpt-5.2",
      "temperature": 0.1,
      "mode": "subagent",
      "description": "CoPilot: Elite reasoning - GPT-5.2"
    },
    
    "copilot-rapid": {
      "model": "github-copilot/gpt-5-mini",
      "temperature": 0.2,
      "mode": "subagent",
      "description": "CoPilot: Rapid tasks - GPT-5 mini"
    },
    
    "copilot-multimodal": {
      "model": "github-copilot/gpt-5-mini",
      "temperature": 0.2,
      "mode": "subagent",
      "description": "CoPilot: Multimodal - GPT-5 mini"
    },
    
    "copilot-explorer": {
      "model": "github-copilot/gpt-5-mini-preview",
      "temperature": 0.0,
      "mode": "subagent",
      "description": "CoPilot: Codebase exploration - GPT-5 mini"
    },
    
    "copilot-librarian": {
      "model": "github-copilot/gpt-5-mini-preview",
      "temperature": 0.1,
      "mode": "subagent",
      "description": "CoPilot: Research - GPT-5 mini"
    },
    
    // ========================================================================
    // PERSONAL AGENTS
    // ========================================================================
    "__comment_personal": "Personal project agents",
    
    "personal-rapid": {
      "model": "github-copilot/gpt-5-mini",
      "temperature": 0.2,
      "mode": "subagent",
      "description": "Personal: Rapid tasks - GPT-5 mini"
    },
    
    "personal-multimodal": {
      "model": "github-copilot/gpt-5-mini",
      "temperature": 0.2,
      "mode": "subagent",
      "description": "Personal: Multimodal - GPT-5 mini"
    },
    
    "personal-explorer": {
      "model": "github-copilot/gpt-5-mini-preview",
      "temperature": 0.0,
      "mode": "subagent",
      "description": "Personal: Codebase exploration - GPT-5 mini"
    },
    
    "personal-librarian": {
      "model": "github-copilot/gpt-5-mini-preview",
      "temperature": 0.1,
      "mode": "subagent",
      "description": "Personal: Research - GPT-5 mini"
    },
    
    "personal-frontend": {
      "model": "github-copilot/gpt-5-mini-preview",
      "temperature": 0.7,
      "mode": "subagent",
      "description": "Personal: UI/UX - GPT-5 mini"
    },
    
    "personal-documenter": {
      "model": "github-copilot/gpt-5-mini-preview",
      "temperature": 0.5,
      "mode": "subagent",
      "description": "Personal: Documentation - READMEs, guides, API docs"
    },
    
    "__comment_language_specialists": "Personal language specialists",
    
    "personal-rust-engineer": {
      "model": "openai/gpt-5.1-codex-max",
      "temperature": 0.2,
      "mode": "subagent",
      "description": "Personal: Rust specialist"
    },
    
    // ========================================================================
    // OPENCODE AGENTS (OpenCode Black Ecosystem)
    // ========================================================================
    "__comment_opencode": "OpenCode Black ecosystem agents",
    
    "OpenCode-Architect": {
      "model": "opencode/gpt-5.2",
      "temperature": 0.2,
      "mode": "primary",
      "description": "OpenCode: ARCHITECT - GPT-5.2 (80% SWE-bench)"
    },
    
    "OpenCode-Coder": {
      "model": "opencode/claude-sonnet-4-5",
      "temperature": 0.2,
      "mode": "primary",
      "description": "OpenCode: CODER - Claude Sonnet 4.5 (77.2% SWE-bench)"
    },
    
    "opencode-reasoner": {
      "model": "opencode/gpt-5.2",
      "temperature": 0.1,
      "mode": "subagent",
      "description": "OpenCode: Elite reasoning - GPT-5.2"
    },
    
    "opencode-rapid": {
      "model": "opencode/gemini-3-flash",
      "temperature": 0.2,
      "mode": "subagent",
      "description": "OpenCode: Rapid tasks - Gemini 3 Flash (78% SWE-bench)"
    },
    
    "opencode-multimodal": {
      "model": "opencode/gemini-3-pro",
      "temperature": 0.2,
      "mode": "subagent",
      "description": "OpenCode: Multimodal - Gemini 3 Pro"
    },
    
    "opencode-explorer": {
      "model": "opencode/gemini-3-flash",
      "temperature": 0.0,
      "mode": "subagent",
      "description": "OpenCode: Codebase exploration - Gemini Flash"
    },
    
    "opencode-librarian": {
      "model": "opencode/gemini-3-flash",
      "temperature": 0.1,
      "mode": "subagent",
      "description": "OpenCode: Research - Gemini Flash"
    },
    
    "opencode-frontend": {
      "model": "opencode/gemini-3-flash",
      "temperature": 0.7,
      "mode": "subagent",
      "description": "OpenCode: UI/UX - Gemini Flash"
    },
    
    "opencode-documenter": {
      "model": "opencode/gemini-3-flash",
      "temperature": 0.5,
      "mode": "subagent",
      "description": "OpenCode: Documentation - Gemini Flash"
    },
    
    "rust-engineer": {
      "model": "opencode/gpt-5.2-codex",
      "temperature": 0.2,
      "mode": "subagent",
      "description": "OpenCode: Rust specialist"
    },
    
    "python-ai": {
      "model": "opencode/gpt-5.2-codex",
      "temperature": 0.2,
      "mode": "subagent",
      "description": "OpenCode: Python ML/AI specialist"
    },
    
    "gamedev-haxe": {
      "model": "opencode/gemini-3-flash",
      "temperature": 0.5,
      "mode": "subagent",
      "description": "OpenCode: Heaps.io specialist"
    },
    
    // ========================================================================
    // OLLAMA AGENTS (Private, Local, Limited Context)
    // ========================================================================
    "__comment_ollama": "100% private local models - RTX 5060ti 16GB (limited context)",
    
    "Ollama-Private": {
      "model": "ollama-local/deepseek-r1:8b",
      "temperature": 0.3,
      "mode": "primary",
      "description": "Ollama: PRIVATE CODER - DeepSeek R1 8B (100% private, limited context)"
    },
    
    "Ollama-Fast": {
      "model": "ollama-local/ministral-3:14b",
      "temperature": 0.2,
      "mode": "primary",
      "description": "Ollama: FAST LOCAL - Ministral 3 14B (quick private tasks)"
    },
    
    "ollama-helper": {
      "model": "ollama-local/deepseek-r1:8b",
      "temperature": 0.3,
      "mode": "subagent",
      "description": "Ollama: Helper for simple private tasks (small context)"
    },
    
    // ========================================================================
    // FREE TIER AGENTS (Fallback)
    // ========================================================================
    "__comment_free": "Free models - OpenCode + Google Free Tier",
    
    "Free-Primary": {
      "model": "opencode/glm-4.7",
      "temperature": 0.3,
      "mode": "primary",
      "description": "Free: PRIMARY CODER - GLM-4.7 (73.8% SWE-bench)"
    },
    
    "Free-Builder": {
      "model": "opencode/glm-4.6",
      "temperature": 0.2,
      "mode": "primary",
      "description": "Free: BUILDER - GLM-4.6"
    },
    
    "Free-Rapid": {
      "model": "opencode/grok-code",
      "temperature": 0.2,
      "mode": "primary",
      "description": "Free: RAPID - Grok Code (70.8% SWE, 296 tok/s)"
    },
    
    "Free-Gemini-Lite": {
      "model": "opencode/gemini-3-flash",
      "temperature": 0.2,
      "mode": "primary",
      "description": "Free: Gemini 3 Flash"
    },
    
    "__comment_free_subagents": "Free tier subagents",
    
    "free-reasoner": {
      "model": "opencode/glm-4.7",
      "temperature": 0.1,
      "mode": "subagent",
      "description": "Free: Reasoning - GLM-4.7 (73.8% SWE + 95.7% AIME)"
    },
    
    "free-tester": {
      "model": "opencode/glm-4.7",
      "temperature": 0.2,
      "mode": "subagent",
      "description": "Free: Test generation"
    },
    
    "free-debugger": {
      "model": "opencode/grok-code",
      "temperature": 0.2,
      "mode": "subagent",
      "description": "Free: Debugging - Grok Code"
    },
    
    "free-optimizer": {
      "model": "opencode/glm-4.7",
      "temperature": 0.2,
      "mode": "subagent",
      "description": "Free: Performance optimization"
    },
    
    "free-reviewer": {
      "model": "opencode/glm-4.7",
      "temperature": 0.2,
      "mode": "subagent",
      "description": "Free: Code review"
    },
    
    "free-qa": {
      "model": "opencode/glm-4.7",
      "temperature": 0.3,
      "mode": "subagent",
      "description": "Free: QA planning"
    }
  },
  
  // ========================================================================
  // USAGE PHILOSOPHY
  // ========================================================================
    "usage_philosophy": {
      "kato_work": "Use Anthropic Claude models exclusively for work projects (OpenCode Black)",
      "copilot_tier": "GitHub Copilot ecosystem - Primary for Pantheon agents (Orchestrator, Oracle, etc.) for maximum usage headroom",
      "opencode_tier": "OpenCode Black ecosystem - GPT-5.2 for reasoning, Sonnet 4.5 for coding, Gemini 3 Flash for speed",
      "ollama_private": "Use local Ollama for 100% private work (limited 16GB context)",
      "free_fallback": "GLM-4.7 (73.8%) best free model, Grok Code (70.8%) for rapid debugging"
    }
}
