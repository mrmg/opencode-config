{
  "$schema": "https://opencode.ai/config.json",
  "theme": "opencode",
  "model": "",
  "autoupdate": true,
  
  // ============================================================================
  // KATO / WORK AGENTS (Anthropic Only)
  // ============================================================================
  "agent": {
    
    // Primary Work Agents - Anthropic via OpenCode Black
    "Kato-Architect": {
      "model": "opencode/claude-opus-4-5",
      "temperature": 0.2,
      "mode": "primary",
      "description": "Kato: ARCHITECT - Claude Opus 4.5 (80.9% SWE-bench)"
    },
    
    "Kato-Coder": {
      "model": "opencode/claude-sonnet-4-5",
      "temperature": 0.2,
      "mode": "primary",
      "description": "Kato: BALANCED CODER - Claude Sonnet 4.5 (77.2% SWE-bench)"
    },
    
    "Kato-Strategist": {
      "model": "opencode/claude-opus-4-5",
      "temperature": 0.2,
      "mode": "primary",
      "description": "Kato: STRATEGIC PLANNING - Claude Opus 4.5"
    },
    
    // Work Subagents - Anthropic
    "kato-reviewer": {
      "model": "opencode/claude-sonnet-4-5",
      "temperature": 0.2,
      "mode": "subagent",
      "description": "Kato: Code review and quality assurance"
    },
    
    "kato-documenter": {
      "model": "opencode/claude-sonnet-4-5",
      "temperature": 0.3,
      "mode": "subagent",
      "description": "Kato: Technical documentation for work projects"
    },
    
    // ============================================================================
    // COPILOT AGENTS (GitHub Copilot Ecosystem)
    // ============================================================================
    
    "CoPilot-Architect": {
      "model": "github-copilot/gpt-5.2",
      "temperature": 0.2,
      "mode": "primary",
      "description": "CoPilot: ARCHITECT - GPT-5.2 (80% SWE-bench)"
    },
    
    "CoPilot-Coder": {
      "model": "github-copilot/claude-sonnet-4.5",
      "temperature": 0.2,
      "mode": "primary",
      "description": "CoPilot: CODER - Claude Sonnet 4.5 (77.2% SWE-bench)"
    },
    
    "copilot-reasoner": {
      "model": "github-copilot/gpt-5.2",
      "temperature": 0.1,
      "mode": "subagent",
      "description": "CoPilot: Elite reasoning - GPT-5.2"
    },
    
    "copilot-rapid": {
      "model": "github-copilot/gpt-5-mini",
      "temperature": 0.2,
      "mode": "subagent",
      "description": "CoPilot: Rapid tasks - GPT-5 mini"
    },
    
    "copilot-multimodal": {
      "model": "github-copilot/gpt-5-mini",
      "temperature": 0.2,
      "mode": "subagent",
      "description": "CoPilot: Multimodal - GPT-5 mini"
    },
    
    "copilot-explorer": {
      "model": "github-copilot/gpt-5-mini-preview",
      "temperature": 0.0,
      "mode": "subagent",
      "description": "CoPilot: Codebase exploration - GPT-5 mini"
    },
    
    "copilot-librarian": {
      "model": "github-copilot/gpt-5-mini-preview",
      "temperature": 0.1,
      "mode": "subagent",
      "description": "CoPilot: Research - GPT-5 mini"
    },
    
    // ============================================================================
    // PERSONAL AGENTS
    // ============================================================================
    
    "personal-rapid": {
      "model": "github-copilot/gpt-5-mini",
      "temperature": 0.2,
      "mode": "subagent",
      "description": "Personal: Rapid tasks - GPT-5 mini"
    },
    
    "personal-multimodal": {
      "model": "github-copilot/gpt-5-mini",
      "temperature": 0.2,
      "mode": "subagent",
      "description": "Personal: Multimodal - GPT-5 mini"
    },
    
    "personal-explorer": {
      "model": "github-copilot/gpt-5-mini-preview",
      "temperature": 0.0,
      "mode": "subagent",
      "description": "Personal: Codebase exploration - GPT-5 mini"
    },
    
    "personal-librarian": {
      "model": "github-copilot/gpt-5-mini-preview",
      "temperature": 0.1,
      "mode": "subagent",
      "description": "Personal: Research - GPT-5 mini"
    },
    
    "personal-frontend": {
      "model": "github-copilot/gpt-5-mini-preview",
      "temperature": 0.7,
      "mode": "subagent",
      "description": "Personal: UI/UX - GPT-5 mini"
    },
    
    "personal-documenter": {
      "model": "github-copilot/gpt-5-mini-preview",
      "temperature": 0.5,
      "mode": "subagent",
      "description": "Personal: Documentation - READMEs, guides, API docs"
    },
    
    "personal-rust-engineer": {
      "model": "openai/gpt-5.1-codex-max",
      "temperature": 0.2,
      "mode": "subagent",
      "description": "Personal: Rust specialist"
    },
    
    // ============================================================================
    // OPENCODE AGENTS (OpenCode Black Ecosystem)
    // ============================================================================
    
    "OpenCode-Architect": {
      "model": "opencode/gpt-5.2",
      "temperature": 0.2,
      "mode": "primary",
      "description": "OpenCode: ARCHITECT - GPT-5.2 (80% SWE-bench)"
    },
    
    "OpenCode-Coder": {
      "model": "opencode/claude-sonnet-4-5",
      "temperature": 0.2,
      "mode": "primary",
      "description": "OpenCode: CODER - Claude Sonnet 4.5 (77.2% SWE-bench)"
    },
    
    "opencode-reasoner": {
      "model": "opencode/gpt-5.2",
      "temperature": 0.1,
      "mode": "subagent",
      "description": "OpenCode: Elite reasoning - GPT-5.2"
    },
    
    "opencode-rapid": {
      "model": "opencode/gemini-3-flash-preview",
      "temperature": 0.2,
      "mode": "subagent",
      "description": "OpenCode: Rapid tasks - Gemini 3 Flash (78% SWE-bench)"
    },
    
    "opencode-multimodal": {
      "model": "opencode/gemini-3-pro-preview",
      "temperature": 0.2,
      "mode": "subagent",
      "description": "OpenCode: Multimodal - Gemini 3 Pro"
    },
    
    "opencode-explorer": {
      "model": "opencode/gemini-3-flash-preview",
      "temperature": 0.0,
      "mode": "subagent",
      "description": "OpenCode: Codebase exploration - Gemini Flash"
    },
    
    "opencode-librarian": {
      "model": "opencode/gemini-3-flash-preview",
      "temperature": 0.1,
      "mode": "subagent",
      "description": "OpenCode: Research - Gemini Flash"
    },
    
    "opencode-frontend": {
      "model": "opencode/gemini-3-flash-preview",
      "temperature": 0.7,
      "mode": "subagent",
      "description": "OpenCode: UI/UX - Gemini Flash"
    },
    
    "opencode-documenter": {
      "model": "opencode/gemini-3-flash-preview",
      "temperature": 0.5,
      "mode": "subagent",
      "description": "OpenCode: Documentation - Gemini Flash"
    },
    
    "rust-engineer": {
      "model": "opencode/gpt-5.2-codex",
      "temperature": 0.2,
      "mode": "subagent",
      "description": "OpenCode: Rust specialist"
    },
    
    "python-ai": {
      "model": "opencode/gpt-5.2-codex",
      "temperature": 0.2,
      "mode": "subagent",
      "description": "OpenCode: Python ML/AI specialist"
    },
    
    "gamedev-haxe": {
      "model": "opencode/gemini-3-flash-preview",
      "temperature": 0.5,
      "mode": "subagent",
      "description": "OpenCode: Heaps.io specialist"
    },
    
    // ============================================================================
    // OLLAMA AGENTS (Private, Local, Limited Context)
    // ============================================================================
    
    "Ollama-Private": {
      "model": "ollama-local/deepseek-r1:8b",
      "temperature": 0.3,
      "mode": "primary",
      "description": "Ollama: PRIVATE CODER - DeepSeek R1 8B (100% private, local, limited 16GB context)"
    },
    
    "Ollama-Fast": {
      "model": "ollama-local/ministral-3:14b",
      "temperature": 0.2,
      "mode": "primary",
      "description": "Ollama: FAST LOCAL - Ministral 3 14B (quick local tasks, private)"
    },
    
    "ollama-helper": {
      "model": "ollama-local/deepseek-r1:8b",
      "temperature": 0.3,
      "mode": "subagent",
      "description": "Ollama: Helper agent for simple private tasks (small context)"
    },
    
    // ============================================================================
    // FREE TIER AGENTS (Fallback)
    // ============================================================================
    
    "Free-Primary": {
      "model": "opencode/glm-4.7",
      "temperature": 0.3,
      "mode": "primary",
      "description": "Free: PRIMARY CODER - GLM-4.7 (73.8% SWE-bench)"
    },
    
    "Free-Builder": {
      "model": "opencode/glm-4.6",
      "temperature": 0.2,
      "mode": "primary",
      "description": "Free: BUILDER - GLM-4.6"
    },
    
    "Free-Rapid": {
      "model": "opencode/grok-code",
      "temperature": 0.2,
      "mode": "primary",
      "description": "Free: RAPID CODER - Grok Code (70.8% SWE, 296 tok/s)"
    },
    
    // Free Subagents
    "free-reasoner": {
      "model": "opencode/glm-4.7",
      "temperature": 0.1,
      "mode": "subagent",
      "description": "Free: Elite reasoning - GLM-4.7 (73.8% SWE + 95.7% AIME)"
    },
    
    "free-tester": {
      "model": "opencode/glm-4.7",
      "temperature": 0.2,
      "mode": "subagent",
      "description": "Free: Test generation - unit tests, integration tests"
    },
    
    "free-debugger": {
      "model": "opencode/grok-code",
      "temperature": 0.2,
      "mode": "subagent",
      "description": "Free: Fast debugging - Grok Code (296 tok/s)"
    },
    
    "free-optimizer": {
      "model": "opencode/glm-4.7",
      "temperature": 0.2,
      "mode": "subagent",
      "description": "Free: Performance optimization - profiling, bottlenecks"
    },
    
    "free-reviewer": {
      "model": "opencode/glm-4.7",
      "temperature": 0.2,
      "mode": "subagent",
      "description": "Free: Code review - pattern matching, best practices"
    },
    
    "free-qa": {
      "model": "opencode/glm-4.7",
      "temperature": 0.3,
      "mode": "subagent",
      "description": "Free: QA planning - test planning, edge cases"
    }
  },
  
  // ============================================================================
  // PLUGINS
  // ============================================================================
  "plugin": [
    "oh-my-opencode-slim@latest",
    "opencode-anthropic-auth@latest",
    "opencode-openai-codex-auth@latest",
    "opencode-gemini-auth",
    "yet-another-opencode-cursor-auth"
  ],
  
  // ============================================================================
  // MCP SERVERS
  // ============================================================================
  "mcp": {
    "chrome-devtools": {
      "type": "local",
      "command": ["npx", "-y", "chrome-devtools-mcp@latest", "--isolated=true"]
    },
    "sequential-thinking": {
      "type": "local",
      "command": [
        "npx",
        "-y",
        "@modelcontextprotocol/server-sequential-thinking"
      ]
    },
    "atlassian": {
      "type": "local",
      "command": [
        "npx",
        "-y",
        "mcp-remote",
        "https://mcp.atlassian.com/v1/sse"
      ]
    }
  },
  
  // ============================================================================
  // PROVIDERS
  // ============================================================================
  "provider": {
    "cursor": {
      "name": "Cursor"
    },
    "ollama-local": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Ollama (LAN)",
      "options": {
        "baseURL": "http://192.168.0.131:11434/v1"
      },
      "models": {
        "deepseek-r1:8b": {
          "name": "DeepSeek R1 8B (local)"
        },
        "llama3.2:latest": {
          "name": "Ben's' Model from 1947 (local)"
        },
        "ministral-3:14b": {
          "name": "Ministral 3 14B (local)"
        },
        "sam860/LFM2:1.2b": {
          "name": "LFM2:1.2b (local)"
        },
        "arifulislamat/snoop-dogg:4b": {
          "name": "Snoop Dogg:4b (local)"
        },
        "MrScratchcat22/GLM-4.7-Flash-REAP-23B-A3B:latest": {
          "name": "GLM-4.7-Flash-REAP-23B-A3B:latest (local)"
        }
      }
    }
  },
  
  // ============================================================================
  // SERVER CONFIG (for oh-my-opencode-slim tmux integration)
  // ============================================================================
  "server": {
    "port": 4096
  }
}
