{"type":"user","timestamp":"2026-01-20T07:03:00.776Z","content":"Quick research: provide concise actionable info about the Ollama model identifier 'qwen-3:4b' (or variants like 'qwen3:4b' or 'qwen-3:4b'). Specifically:\n1) Common Ollama model naming patterns and which variant is most likely accepted.\n2) Typical VRAM/CPU footprint for a 4B parameter model (approximate), and whether a 2.5GB measurement is realistic (for 4B likely >6GB; 2.5GB implies 3B or quantized model).\n3) Suggest candidate smaller models (~2â€“3GB or less) available in Ollama or OSS (names/examples) that are practical for prompt-enhancement alongside image gen.\n4) Sample minimal request to Ollama /api/generate for prompt-enhancement (JSON body) and any parameters to limit memory use (stream:false, max_tokens, temperature).\nReturn concise recommendations and model-name caution notes."}
