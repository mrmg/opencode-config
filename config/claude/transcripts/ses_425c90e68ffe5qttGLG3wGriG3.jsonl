{"type":"user","timestamp":"2026-01-20T07:03:12.536Z","content":"Research and provide concise actionable info about Ollama model identifiers and small models for prompt-enhancement alongside image generation.\n\nTasks:\n1) Clarify common Ollama model naming variants: e.g., 'qwen3:4b', 'qwen-3:4b', 'qwen-3.4b' — which format is typically accepted by Ollama's /api/generate 'model' field? Mention if both with/without hyphen are seen and advise best practice (use exact model name from `ollama ls` output or env override).\n2) Provide realistic VRAM/footprint expectations for a 4B parameter model vs a truly 2.5GB model. Explain how quantization (int4/int8) or Q4_K_M etc affects RAM and why a quoted 2.5GB could be a quantized variant — approximate numbers.\n3) Recommend candidate smaller models (~2–3GB or less) that are practical to run alongside image generation. Prefer names commonly distributed for Ollama or OSS (e.g., vicuna-3b, lml6b? include quantized versions like qwen-3-small-int4 if known). If exact Ollama package names are uncertain, say to prefer checking `ollama ls` and recommend well-known small models: vicuna-3b-v1.5, llama-2-7b-q4? (note sizes). Emphasize using quantized variants (int4/int8) to reach ~2-3GB.\n4) Provide a minimal JSON request body example to POST to Ollama /api/generate for prompt enhancement, including safe params to limit memory/time: { model, prompt, system, stream:false, max_tokens: 200, temperature:0.2 }. Note any model-specific params that might be accepted (max_tokens, temperature, top_p) and recommend defaults.\n5) Give quick practical advice: how to detect if model is installed and its name (`ollama ls`), and fallback approach: environment variable OLLAMA_MODEL override and a server startup warning if model not found.\n\nReturn a short actionable recommendation: if your smallest local model is reported as 'qwen-3:4b' at ~2.5GB, warn that naming may differ and that it might be a quantized/packaged build; advise to run `ollama ls` and try the model with a small test request using low max_tokens and stream:false before enabling feature.\n\nKeep answer concise (bullet points ok)."}
